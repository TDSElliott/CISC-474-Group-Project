{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import constant as const\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Callable, Dict, List, Any, Optional, TypeVar\n",
    "\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "import tf_agents\n",
    "\n",
    "T = TypeVar(\"T\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def define_rl_environment() -> TFEnvironment:\n",
    "    env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "    const.DATA_PATH, const.RANK_K, const.BATCH_SIZE, num_movies=const.NUM_ACTIONS, csv_delimiter=\"\\t\")\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "    return environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def define_rl_agent(environment: TFEnvironment) -> TFAgent:\n",
    "    # Define RL agent/algorithm.\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=environment.time_step_spec(),\n",
    "        action_spec=environment.action_spec(),\n",
    "        tikhonov_weight=const.TIKHONOV_WEIGHT,\n",
    "        alpha=const.AGENT_ALPHA,\n",
    "        dtype=tf.float32,\n",
    "        accepts_per_arm_features=const.PER_ARM)\n",
    "    return agent\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def define_rl_metric(environment: TFEnvironment) -> List[Any]:\n",
    "    optimal_reward_fn = functools.partial(\n",
    "    environment_utilities.compute_optimal_reward_with_movielens_environment,\n",
    "    environment=environment)\n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    metrics = [regret_metric]\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(\n",
    "    root_dir: str,\n",
    "    agent: TFAgent,\n",
    "    environment: TFEnvironment,\n",
    "    training_loops: int,\n",
    "    steps_per_loop: int,\n",
    "    additional_metrics: Optional[List[TFStepMetric]] = None,\n",
    "    training_data_spec_transformation_fn: Optional[Callable[[T], T]] = None,\n",
    ") -> Dict[str, List[float]]:\n",
    "  \"\"\"Performs `training_loops` iterations of training on the agent's policy.\n",
    "  Uses the `environment` as the problem formulation and source of immediate\n",
    "  feedback and the agent's algorithm, to perform `training-loops` iterations\n",
    "  of on-policy training on the policy.\n",
    "  If one or more baseline_reward_fns are provided, the regret is computed\n",
    "  against each one of them. Here is example baseline_reward_fn:\n",
    "  def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "   rewards = ... # compute reward for each arm\n",
    "   optimal_action_reward = ... # take the maximum reward\n",
    "   return optimal_action_reward\n",
    "  Args:\n",
    "    root_dir: Path to the directory where training artifacts are written.\n",
    "    agent: An instance of `TFAgent`.\n",
    "    environment: An instance of `TFEnvironment`.\n",
    "    training_loops: An integer indicating how many training loops should be run.\n",
    "    steps_per_loop: An integer indicating how many driver steps should be\n",
    "      executed and presented to the trainer during each training loop.\n",
    "    additional_metrics: Optional; list of metric objects to log, in addition to\n",
    "      default metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "      `AverageEpisodeLengthMetric`.\n",
    "    training_data_spec_transformation_fn: Optional; function that transforms\n",
    "      the data items before they get to the replay buffer.\n",
    "  Returns:\n",
    "    A dict mapping metric names (eg. \"AverageReturnMetric\") to a list of\n",
    "    intermediate metric values over `training_loops` iterations of training.\n",
    "  \"\"\"\n",
    "  if training_data_spec_transformation_fn is None:\n",
    "    data_spec = agent.policy.trajectory_spec\n",
    "  else:\n",
    "    data_spec = training_data_spec_transformation_fn(\n",
    "        agent.policy.trajectory_spec)\n",
    "  replay_buffer = trainer.get_replay_buffer(data_spec, environment.batch_size,\n",
    "                                            steps_per_loop)\n",
    "\n",
    "  # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "  # that is, (number of trajectories) * batch_size.\n",
    "  step_metric = tf_metrics.EnvironmentSteps()\n",
    "  metrics = [\n",
    "      tf_metrics.NumberOfEpisodes(),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n",
    "  ]\n",
    "  if additional_metrics:\n",
    "    metrics += additional_metrics\n",
    "\n",
    "  if isinstance(environment.reward_spec(), dict):\n",
    "    metrics += [tf_metrics.AverageReturnMultiMetric(\n",
    "        reward_spec=environment.reward_spec(),\n",
    "        batch_size=environment.batch_size)]\n",
    "  else:\n",
    "    metrics += [\n",
    "        tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)]\n",
    "\n",
    "  # Store intermediate metric results, indexed by metric names.\n",
    "  metric_results = defaultdict(list)\n",
    "\n",
    "  if training_data_spec_transformation_fn is not None:\n",
    "    add_batch_fn = lambda data: replay_buffer.add_batch(\n",
    "        training_data_spec_transformation_fn(data))\n",
    "  else:\n",
    "    add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "  observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "  driver = dynamic_step_driver.DynamicStepDriver(\n",
    "      env=environment,\n",
    "      policy=agent.collect_policy,\n",
    "      num_steps=steps_per_loop * environment.batch_size,\n",
    "      observers=observers)\n",
    "\n",
    "  training_loop = trainer.get_training_loop_fn(\n",
    "      driver, replay_buffer, agent, steps_per_loop)\n",
    "  saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "  for _ in range(training_loops):\n",
    "    training_loop()\n",
    "    metric_utils.log_metrics(metrics)\n",
    "    for metric in metrics:\n",
    "      metric.tf_summaries(train_step=step_metric.result())\n",
    "      metric_results[type(metric).__name__].append(metric.result().numpy())\n",
    "  saver.save(root_dir)\n",
    "  return metric_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def predict_observations_by_users(observation: List[List[float]]) -> List[int]:\n",
    "    # Trained policy is saved in the ROOT_DIR, given observation\n",
    "    # outputs action.\n",
    "    # https://www.tensorflow.org/agents/tutorials/\n",
    "    # 10_checkpointer_policysaver_tutorial#restore_checkpoint\n",
    "    trained_policy = tf.saved_model.load(const.ROOT_DIR)\n",
    "\n",
    "\n",
    "    # reference: https://github.com/yutsai84/vertex-ai-samples/\n",
    "    # blob/ee6dd357320a9fb875750331c2558b510c8b316f/community-content/\n",
    "    # tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/\n",
    "    # step_by_step_sdk_tf_agents_bandits_movie_recommendation/src/\n",
    "    # prediction/main.py#L60-L63\n",
    "\n",
    "    time_step = tf_agents.trajectories.restart(\n",
    "        observation=observation,\n",
    "        batch_size=tf.convert_to_tensor([const.BATCH_SIZE]),\n",
    "    )\n",
    "    action_step = trained_policy.action(time_step)\n",
    "    return action_step.action.numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "env = define_rl_environment()\n",
    "agent = define_rl_agent(env)\n",
    "metric = define_rl_metric(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "TensorSpec(shape=(20,), dtype=tf.float64, name='observation')\n",
      "Reward spec\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "Action spec\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(19))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "\n",
    "print('Reward spec')\n",
    "print(env.reward_spec())\n",
    "\n",
    "print('Action spec')\n",
    "print(env.action_spec())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function TFStepMetric._update_state at 0x000001B3725AAA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function TFStepMetric._update_state at 0x000001B3725AAA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: H:/Users/Shane/Documents/GitHub/474_Group_Project/artifacts\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\nested_structure_coder.py:559: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: H:/Users/Shane/Documents/GitHub/474_Group_Project/artifacts\\assets\n"
     ]
    }
   ],
   "source": [
    "metrics = train(const.ROOT_DIR, agent, env, const.TRAINING_LOOPS, const.STEPS_PER_LOOP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * TimeStep(\n{'discount': <tf.Tensor 'time_step_2:0' shape=(8,) dtype=float32>,\n 'observation': <tf.Tensor 'observation:0' shape=(20,) dtype=float64>,\n 'reward': <tf.Tensor 'time_step_1:0' shape=(8,) dtype=float32>,\n 'step_type': <tf.Tensor 'time_step:0' shape=(8,) dtype=int32>})\n    * ()\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='observation'))\n    * ()\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='time_step/observation'))\n    * ()\n  Keyword arguments: {}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/3948240998.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mactions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredict_observations_by_users\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime_step_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mactions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/1509587441.py\u001B[0m in \u001B[0;36mpredict_observations_by_users\u001B[1;34m(observation)\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mconst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     )\n\u001B[1;32m---> 19\u001B[1;33m     \u001B[0maction_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained_policy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0maction_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\u001B[0m in \u001B[0;36mrestored_function_body\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m           \u001B[1;34m\"Option {}:\\n  {}\\n  Keyword arguments: {}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m           .format(index + 1, _pretty_format_positional(positional), keyword))\n\u001B[1;32m--> 289\u001B[1;33m     raise ValueError(\n\u001B[0m\u001B[0;32m    290\u001B[0m         \u001B[1;34m\"Could not find matching concrete function to call loaded from the \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m         \u001B[1;34mf\"SavedModel. Got:\\n  {_pretty_format_positional(args)}\\n  Keyword \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * TimeStep(\n{'discount': <tf.Tensor 'time_step_2:0' shape=(8,) dtype=float32>,\n 'observation': <tf.Tensor 'observation:0' shape=(20,) dtype=float64>,\n 'reward': <tf.Tensor 'time_step_1:0' shape=(8,) dtype=float32>,\n 'step_type': <tf.Tensor 'time_step:0' shape=(8,) dtype=int32>})\n    * ()\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='observation'))\n    * ()\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='time_step/observation'))\n    * ()\n  Keyword arguments: {}"
     ]
    }
   ],
   "source": [
    "actions = predict_observations_by_users(env.time_step_spec().observation)\n",
    "print(actions)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(19))\n",
      "time_step_spec.observation: TensorSpec(shape=(20,), dtype=tf.float64, name='observation')\n",
      "time_step_spec.step_type: TensorSpec(shape=(), dtype=tf.int32, name='step_type')\n",
      "time_step_spec.discount: BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))\n",
      "time_step_spec.reward: TensorSpec(shape=(), dtype=tf.float32, name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation:', env.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', env.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', env.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', env.time_step_spec().reward)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}