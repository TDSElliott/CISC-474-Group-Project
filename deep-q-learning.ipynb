{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "\n",
    "\n",
    "import constant as const\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Callable, Dict, List, Any, Optional, TypeVar\n",
    "\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver, py_tf_eager_policy\n",
    "import tf_agents\n",
    "from tf_agents.drivers import py_driver\n",
    "\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def define_rl_environment() -> TFEnvironment:\n",
    "    env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "    const.DATA_PATH, const.RANK_K, const.BATCH_SIZE, num_movies=const.NUM_ACTIONS, csv_delimiter=\"\\t\")\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "    return environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def define_rl_agent(environment: TFEnvironment) -> TFAgent:\n",
    "    # Define RL agent/algorithm.\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=environment.time_step_spec(),\n",
    "        action_spec=environment.action_spec(),\n",
    "        tikhonov_weight=const.TIKHONOV_WEIGHT,\n",
    "        alpha=const.AGENT_ALPHA,\n",
    "        dtype=tf.float32,\n",
    "        accepts_per_arm_features=const.PER_ARM)\n",
    "    return agent\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def define_rl_metric(environment: TFEnvironment) -> List[Any]:\n",
    "    optimal_reward_fn = functools.partial(\n",
    "    environment_utilities.compute_optimal_reward_with_movielens_environment,\n",
    "    environment=environment)\n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    metrics = [regret_metric]\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(\n",
    "    root_dir: str,\n",
    "    agent: TFAgent,\n",
    "    environment: TFEnvironment,\n",
    "    training_loops: int,\n",
    "    steps_per_loop: int,\n",
    "    additional_metrics: Optional[List[TFStepMetric]] = None,\n",
    "    training_data_spec_transformation_fn: Optional[Callable[[T], T]] = None,\n",
    ") -> Dict[str, List[float]]:\n",
    "  \"\"\"Performs `training_loops` iterations of training on the agent's policy.\n",
    "  Uses the `environment` as the problem formulation and source of immediate\n",
    "  feedback and the agent's algorithm, to perform `training-loops` iterations\n",
    "  of on-policy training on the policy.\n",
    "  If one or more baseline_reward_fns are provided, the regret is computed\n",
    "  against each one of them. Here is example baseline_reward_fn:\n",
    "  def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "   rewards = ... # compute reward for each arm\n",
    "   optimal_action_reward = ... # take the maximum reward\n",
    "   return optimal_action_reward\n",
    "  Args:\n",
    "    root_dir: Path to the directory where training artifacts are written.\n",
    "    agent: An instance of `TFAgent`.\n",
    "    environment: An instance of `TFEnvironment`.\n",
    "    training_loops: An integer indicating how many training loops should be run.\n",
    "    steps_per_loop: An integer indicating how many driver steps should be\n",
    "      executed and presented to the trainer during each training loop.\n",
    "    additional_metrics: Optional; list of metric objects to log, in addition to\n",
    "      default metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "      `AverageEpisodeLengthMetric`.\n",
    "    training_data_spec_transformation_fn: Optional; function that transforms\n",
    "      the data items before they get to the replay buffer.\n",
    "  Returns:\n",
    "    A dict mapping metric names (eg. \"AverageReturnMetric\") to a list of\n",
    "    intermediate metric values over `training_loops` iterations of training.\n",
    "  \"\"\"\n",
    "  if training_data_spec_transformation_fn is None:\n",
    "    data_spec = agent.policy.trajectory_spec\n",
    "  else:\n",
    "    data_spec = training_data_spec_transformation_fn(\n",
    "        agent.policy.trajectory_spec)\n",
    "  replay_buffer = trainer.get_replay_buffer(data_spec, environment.batch_size,\n",
    "                                            steps_per_loop)\n",
    "\n",
    "  # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "  # that is, (number of trajectories) * batch_size.\n",
    "  step_metric = tf_metrics.EnvironmentSteps()\n",
    "  metrics = [\n",
    "      tf_metrics.NumberOfEpisodes(),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n",
    "  ]\n",
    "  if additional_metrics:\n",
    "    metrics += additional_metrics\n",
    "\n",
    "  if isinstance(environment.reward_spec(), dict):\n",
    "    metrics += [tf_metrics.AverageReturnMultiMetric(\n",
    "        reward_spec=environment.reward_spec(),\n",
    "        batch_size=environment.batch_size)]\n",
    "  else:\n",
    "    metrics += [\n",
    "        tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)]\n",
    "\n",
    "  # Store intermediate metric results, indexed by metric names.\n",
    "  metric_results = defaultdict(list)\n",
    "\n",
    "  if training_data_spec_transformation_fn is not None:\n",
    "    add_batch_fn = lambda data: replay_buffer.add_batch(\n",
    "        training_data_spec_transformation_fn(data))\n",
    "  else:\n",
    "    add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "  observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "  driver = dynamic_step_driver.DynamicStepDriver(\n",
    "      env=environment,\n",
    "      policy=agent.collect_policy,\n",
    "      num_steps=steps_per_loop * environment.batch_size,\n",
    "      observers=observers)\n",
    "\n",
    "  training_loop = trainer.get_training_loop_fn(\n",
    "      driver, replay_buffer, agent, steps_per_loop)\n",
    "  saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "  for _ in range(training_loops):\n",
    "    training_loop()\n",
    "    metric_utils.log_metrics(metrics)\n",
    "    for metric in metrics:\n",
    "      metric.tf_summaries(train_step=step_metric.result())\n",
    "      metric_results[type(metric).__name__].append(metric.result().numpy())\n",
    "  saver.save(root_dir)\n",
    "  return metric_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def predict_observations_by_users(observation: List[List[float]]) -> List[int]:\n",
    "    # Trained policy is saved in the ROOT_DIR, given observation\n",
    "    # outputs action.\n",
    "    # https://www.tensorflow.org/agents/tutorials/\n",
    "    # 10_checkpointer_policysaver_tutorial#restore_checkpoint\n",
    "    trained_policy = tf.saved_model.load(const.ROOT_DIR)\n",
    "\n",
    "\n",
    "    # reference: https://github.com/yutsai84/vertex-ai-samples/\n",
    "    # blob/ee6dd357320a9fb875750331c2558b510c8b316f/community-content/\n",
    "    # tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/\n",
    "    # step_by_step_sdk_tf_agents_bandits_movie_recommendation/src/\n",
    "    # prediction/main.py#L60-L63\n",
    "\n",
    "    time_step = tf_agents.trajectories.restart(\n",
    "        observation=observation,\n",
    "        batch_size=tf.convert_to_tensor([const.BATCH_SIZE]),\n",
    "    )\n",
    "    action_step = trained_policy.action(time_step)\n",
    "    return action_step.action.numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "env = define_rl_environment()\n",
    "agent = define_rl_agent(env)\n",
    "metric = define_rl_metric(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 14 15  0 13 13 18  7]\n",
      "[5. 5. 5. 5. 5. 4. 3. 5.]\n",
      "TensorSpec(shape=(20,), dtype=tf.float64, name='observation')\n"
     ]
    }
   ],
   "source": [
    "print(env.compute_optimal_action())\n",
    "print(env.compute_optimal_reward())\n",
    "print(env.observation_spec())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "TensorSpec(shape=(20,), dtype=tf.float64, name='observation')\n",
      "Reward spec\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "Action spec\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(19))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "\n",
    "print('Reward spec')\n",
    "print(env.reward_spec())\n",
    "\n",
    "print('Action spec')\n",
    "print(env.action_spec())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function TFStepMetric._update_state at 0x000001B3725AAA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function TFStepMetric._update_state at 0x000001B3725AAA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: H:/Users/Shane/Documents/GitHub/474_Group_Project/artifacts\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\nested_structure_coder.py:559: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: H:/Users/Shane/Documents/GitHub/474_Group_Project/artifacts\\assets\n"
     ]
    }
   ],
   "source": [
    "metrics = train(const.ROOT_DIR, agent, env, const.TRAINING_LOOPS, const.STEPS_PER_LOOP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * TimeStep(\n{'discount': <tf.Tensor 'time_step_2:0' shape=(8,) dtype=float32>,\n 'observation': <tf.Tensor 'observation:0' shape=(20,) dtype=float64>,\n 'reward': <tf.Tensor 'time_step_1:0' shape=(8,) dtype=float32>,\n 'step_type': <tf.Tensor 'time_step:0' shape=(8,) dtype=int32>})\n    * ()\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='observation'))\n    * ()\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='time_step/observation'))\n    * ()\n  Keyword arguments: {}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/3948240998.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mactions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredict_observations_by_users\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime_step_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mactions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/1509587441.py\u001B[0m in \u001B[0;36mpredict_observations_by_users\u001B[1;34m(observation)\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mconst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     )\n\u001B[1;32m---> 19\u001B[1;33m     \u001B[0maction_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained_policy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0maction_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\u001B[0m in \u001B[0;36mrestored_function_body\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m           \u001B[1;34m\"Option {}:\\n  {}\\n  Keyword arguments: {}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m           .format(index + 1, _pretty_format_positional(positional), keyword))\n\u001B[1;32m--> 289\u001B[1;33m     raise ValueError(\n\u001B[0m\u001B[0;32m    290\u001B[0m         \u001B[1;34m\"Could not find matching concrete function to call loaded from the \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m         \u001B[1;34mf\"SavedModel. Got:\\n  {_pretty_format_positional(args)}\\n  Keyword \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * TimeStep(\n{'discount': <tf.Tensor 'time_step_2:0' shape=(8,) dtype=float32>,\n 'observation': <tf.Tensor 'observation:0' shape=(20,) dtype=float64>,\n 'reward': <tf.Tensor 'time_step_1:0' shape=(8,) dtype=float32>,\n 'step_type': <tf.Tensor 'time_step:0' shape=(8,) dtype=int32>})\n    * ()\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='observation'))\n    * ()\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='time_step/observation'))\n    * ()\n  Keyword arguments: {}"
     ]
    }
   ],
   "source": [
    "actions = predict_observations_by_users(env.time_step_spec().observation)\n",
    "print(actions)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(19))\n",
      "time_step_spec.observation: TensorSpec(shape=(20,), dtype=tf.float64, name='observation')\n",
      "time_step_spec.step_type: TensorSpec(shape=(), dtype=tf.int32, name='step_type')\n",
      "time_step_spec.discount: BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))\n",
      "time_step_spec.reward: TensorSpec(shape=(), dtype=tf.float32, name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation:', env.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', env.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', env.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', env.time_step_spec().reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Try tensorflow Deep Q Network tutorial\n",
    "\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "a = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "a.initialize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function TFStepMetric._update_state at 0x000001B37255B790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function TFStepMetric._update_state at 0x000001B37255B790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From h:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From h:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "h:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\nested_structure_coder.py:559: UserWarning: Encoding a StructuredValue with type tf_agents.policies.greedy_policy.DeterministicWithLogProb_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: H:/Users/Shane/Documents/GitHub/474_Group_Project/artifacts\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: H:/Users/Shane/Documents/GitHub/474_Group_Project/artifacts\\assets\n"
     ]
    }
   ],
   "source": [
    "metrics = train(const.ROOT_DIR, a, env, const.TRAINING_LOOPS, const.STEPS_PER_LOOP)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * TimeStep(\n{'discount': <tf.Tensor 'time_step_2:0' shape=(8,) dtype=float32>,\n 'observation': <tf.Tensor 'observation:0' shape=(20,) dtype=float64>,\n 'reward': <tf.Tensor 'time_step_1:0' shape=(8,) dtype=float32>,\n 'step_type': <tf.Tensor 'time_step:0' shape=(8,) dtype=int32>})\n    * ()\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='observation'))\n    * ()\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='time_step/observation'))\n    * ()\n  Keyword arguments: {}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/3597701690.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mactions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredict_observations_by_users\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime_step_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/1509587441.py\u001B[0m in \u001B[0;36mpredict_observations_by_users\u001B[1;34m(observation)\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mconst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     )\n\u001B[1;32m---> 19\u001B[1;33m     \u001B[0maction_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained_policy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0maction_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh:\\users\\shane\\documents\\github\\474_group_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\u001B[0m in \u001B[0;36mrestored_function_body\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m           \u001B[1;34m\"Option {}:\\n  {}\\n  Keyword arguments: {}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m           .format(index + 1, _pretty_format_positional(positional), keyword))\n\u001B[1;32m--> 289\u001B[1;33m     raise ValueError(\n\u001B[0m\u001B[0;32m    290\u001B[0m         \u001B[1;34m\"Could not find matching concrete function to call loaded from the \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m         \u001B[1;34mf\"SavedModel. Got:\\n  {_pretty_format_positional(args)}\\n  Keyword \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * TimeStep(\n{'discount': <tf.Tensor 'time_step_2:0' shape=(8,) dtype=float32>,\n 'observation': <tf.Tensor 'observation:0' shape=(20,) dtype=float64>,\n 'reward': <tf.Tensor 'time_step_1:0' shape=(8,) dtype=float32>,\n 'step_type': <tf.Tensor 'time_step:0' shape=(8,) dtype=int32>})\n    * ()\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='observation'))\n    * ()\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 20), dtype=tf.float64, name='time_step/observation'))\n    * ()\n  Keyword arguments: {}"
     ]
    }
   ],
   "source": [
    "actions = predict_observations_by_users(env.time_step_spec().observation)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action\n",
      "tf.Tensor([19 16 14 16 19 16 15 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[11 14 11 14 13 14  8 12]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 5.1571906e-15 -1.2268732e-15  8.5687623e-15 -1.2586946e-15\n",
      "  2.5372878e-15 -4.0126132e-15  2.5550330e-17 -8.9275308e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 15 16 16 16 16 16 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  8 12  3  7 14  0  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-4.19778517e-15  2.55503303e-17 -6.43760922e-15 -1.24330384e-14\n",
      " -1.92789866e-15 -3.04232995e-15 -3.81596432e-15  3.55497780e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 16 16 12 19 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  0 16  3 14  0 18  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-5.27047956e-15 -2.30968648e-15  5.00000000e+00 -1.24330384e-14\n",
      " -5.90721159e-15  3.00000000e+00  4.33269380e-15 -3.47281772e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 12 15 16 15 15 12 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[11 11  8  0  0  6 11  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-1.3289044e-14  1.3327968e-15  1.2138114e-16 -1.4285521e-14\n",
      "  1.8153576e-15  3.5549778e-15  3.1830710e-15  7.3322608e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16  6 16 16 15 15 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[10 10 14  0  6  8  0  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-4.0829635e-15  4.0000000e+00 -9.6432431e-15  3.0000000e+00\n",
      "  3.2502681e-15  6.9894591e-16 -1.8271811e-15 -2.3096865e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([ 1 16 16 16 16 15 15 14], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8 14  1  6  3  8  6  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 3.00000000e+00 -7.14810935e-15 -1.08937704e-14 -9.62881728e-15\n",
      " -1.24330384e-14  2.37510220e-15  1.16554063e-15  9.85641503e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([ 1 16 19 16  6  6 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  6  0 11  0 10  7  8]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 4.0000000e+00 -1.3299978e-14  2.8117434e-15 -1.5303334e-14\n",
      "  3.0000000e+00 -3.4713240e-15 -1.3378337e-14 -1.7428392e-14], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 15 16 16 16  6  1], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8  0  6 19 14  8  3  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.0000000e+00 -3.8650922e-15  1.0466538e-15 -3.4502341e-15\n",
      " -4.1186172e-15 -1.1361867e-14  4.0000000e+00 -6.5176365e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 15 16 16 16 16 15 19], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14  0 14 12 14 12  8  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-3.6195645e-15  4.6502659e-15 -9.9284903e-16 -1.3424140e-15\n",
      " -6.8477659e-15  3.6666037e-16  2.5550330e-17  2.8709626e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 15  6 16 15 16 16 14], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  8  3 11  6  0  6  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-1.0887913e-14 -2.1648976e-16 -2.4262883e-16  3.0000000e+00\n",
      "  1.4437525e-15 -6.0856306e-15 -1.6137992e-14  1.4650020e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 19 15 16 16  6 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8 13  8  7  8 11 11  7]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.0000000e+00  2.7375487e-15  2.0863694e-15  3.0000000e+00\n",
      " -4.5434716e-15  4.0000000e+00 -6.6112031e-15 -7.5049313e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 16  1  1 16 16 15 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 12  6  0 11  0  0  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 1.3876908e-15  7.3332073e-16 -8.3576758e-15 -6.3513364e-15\n",
      " -6.9835986e-15 -5.3025990e-15  1.8153576e-15  1.7776068e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 16 16 15  1 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 6 14 14  9 11 14  0 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-7.3660907e-15 -4.0126132e-15 -8.3173749e-16 -1.6503070e-14\n",
      "  3.0000000e+00  1.8350209e-15 -6.3513364e-15 -6.1982585e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 15 16 16 16 19 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14 14  8  6 10 14 13 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-7.4213122e-15 -5.1269939e-15  1.5076342e-15 -7.5486949e-15\n",
      " -8.9601043e-15 -1.2586946e-15  3.0000000e+00 -9.2384418e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 15 15 16 15 19  1 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  8  8  8  6 18  7  8]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-7.9233563e-15  3.6015070e-17  1.6093729e-15 -1.6463120e-14\n",
      "  1.3888999e-15  7.1899977e-15  3.0000000e+00  2.0000000e+00], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 16 16  6 19 16 16  6], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8 14  0 11 13 11  8  8]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.5550330e-17 -1.3865568e-14 -1.4285521e-14  3.0000000e+00\n",
      "  9.4935432e-16 -6.9835986e-15 -1.7428392e-14  1.7325730e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15  6 15 16 16 16 19 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8  0 11  0 11  0 11  7]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.5550330e-17 -1.6837638e-15  4.1408391e-15 -6.3485801e-15\n",
      " -1.5303334e-14 -1.1559816e-14  2.2377312e-15 -9.5455044e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 19 15 19 14 14 15 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14  0  8 18  0 10  0 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-3.6195645e-15  4.0000000e+00  1.1887222e-15  9.6473859e-15\n",
      "  1.4650020e-15  2.9155836e-14  1.8153576e-15  6.3577181e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([ 1 19 15 12 16 16 15 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8 13 14 11 14  6  0  8]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 3.00000000e+00 -1.01282595e-16  6.67551377e-16  1.53812056e-15\n",
      " -9.23844182e-15 -2.18203459e-15  1.34787474e-15  9.08160986e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([19  6 16  6 15 15 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[13 16 14  7  0  8 14  7]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.1485548e-15  1.0848590e-15 -2.9326140e-15 -5.8201768e-15\n",
      "  1.3876908e-15  4.9270472e-16 -5.1219129e-15 -9.5455044e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 14 19 16 16 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14  6  9 10  0  0 12  8]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-9.9284903e-16  2.0000000e+00 -1.6503070e-14  2.6454370e-14\n",
      "  5.0000000e+00 -4.9600924e-15  2.9744949e-16  1.4716618e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 16 16 15 19 16 16 19], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 14 11  0  4  7 14 18]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.0412463e-15 -6.5903110e-15  2.0000000e+00  2.4173867e-15\n",
      "  2.8538774e-15 -1.9262004e-14 -1.2586946e-15  3.8587039e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 19 16 16 16 19], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[12  6 10 13 14 14 14 19]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-6.4376092e-15 -8.7302043e-15 -4.0829635e-15  1.4155539e-15\n",
      " -6.8477659e-15 -1.2690137e-14 -4.1316518e-15  4.0000000e+00], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([14 19 16 15 16 19 15  1], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 18 16 11 14 13 14  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 1.4650020e-15  6.9323696e-15  5.0000000e+00  6.2810172e-16\n",
      " -1.0659106e-14  4.2475933e-15  1.2864966e-15  3.0000000e+00], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 16 16 15 15 16 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 6 12 13 16  6 14 14 11]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 1.7041655e-15  3.0000000e+00 -1.0563077e-14  3.9804492e-15\n",
      "  7.3322608e-16 -7.2850382e-15 -4.1914333e-15 -1.1621340e-14], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([19 15 16 16 16 19 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[18 11  0 14  0 18 10 11]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.0000000e+00  4.1408391e-15  4.0000000e+00 -3.6195645e-15\n",
      "  4.0000000e+00  7.1899977e-15  1.0000000e+00 -8.5754166e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([14 16 16 16 16 15 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  9  7  8 14 13  8 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 5.5822694e-15 -2.1964161e-15 -9.5455044e-16 -4.5434716e-15\n",
      " -5.9072116e-15  2.7627136e-15 -1.1361867e-14 -4.0126132e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([ 1 16 16 16 16 16 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 6  8 11 14  0 14  7 12]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-5.3306214e-15 -1.5168115e-14  1.0000000e+00 -1.8932342e-14\n",
      " -1.5943950e-14 -1.4907087e-14 -1.9262004e-14 -2.0009569e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 15 19 16  1 16 15 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14  8  4  3  7 14  8 10]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 6.6755138e-16  2.5550330e-17  5.1433079e-16  2.0000000e+00\n",
      "  3.0000000e+00 -7.0480925e-15  1.9855348e-15 -8.9601043e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 16 12 19 16 19 14 14], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[11  0  0 18 11 13  8 11]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.72111621e-15 -2.30968648e-15  3.19564609e-15  5.00000000e+00\n",
      " -3.34715445e-15 -1.01282595e-16  1.47236214e-14  1.13703619e-14], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 19 16 16 16 16 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14 18 14 14 14 14  3 19]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-3.2086436e-15  9.5646088e-15 -2.9326140e-15 -6.1782854e-15\n",
      " -1.2589716e-14 -9.9284903e-16 -1.0700225e-14 -1.0456709e-14], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 15 16 16 19 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[11  0  0  0 14  7 18 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-3.9524014e-15 -7.3108794e-15  2.0000000e+00 -5.5878347e-17\n",
      " -1.2250346e-14 -2.3292923e-15  7.1899977e-15 -4.2203400e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([14 15 16 19 19 15 16  6], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 14 14 11  4  6  0 12]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 1.46500203e-15  1.36022035e-15 -1.20145195e-14  2.23773124e-15\n",
      "  2.08396892e-15  1.80875627e-15 -7.17302990e-15 -7.67992745e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([19 16  1 15 16 16  1 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[13  7  6  8  7 11  0  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-1.8956781e-16 -2.3292923e-15 -9.0711223e-15  2.5550330e-17\n",
      " -4.7945686e-15 -6.9835986e-15 -1.0134145e-14 -5.4340878e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 16 16 19 16 16 15 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8  0 10 13  7 14  0  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-1.5197304e-16 -6.9581711e-15  4.0000000e+00 -1.8956781e-16\n",
      " -8.1614064e-15 -4.7131645e-15  1.9986284e-15 -4.1629916e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16  6 16 14 16 16  1], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 12  0 14  0 11  9  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 4.00000000e+00 -3.02226226e-15  3.00000000e+00 -7.07285725e-15\n",
      "  8.38427940e-15 -6.98359861e-15 -1.20318125e-14 -6.09324543e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([14 16 16 16  6 15 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 14  0  0  3 14  0 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.2469209e-14 -7.2850382e-15 -3.8650922e-15 -2.3096865e-15\n",
      " -2.4262883e-16  1.2864966e-15 -3.9066024e-15 -5.9821532e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 15 16 19 16 16 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 8  6  0 13  7  3 14 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 3.00000000e+00  1.66943471e-15  2.00000000e+00  1.03289849e-14\n",
      " -1.92620036e-14 -1.24330384e-14 -2.94534193e-15  2.00000000e+00], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([12 19 16 16 15  6 16 19], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[11  5  0  0  8  0 14 13]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 1.5381206e-15  4.8763945e-15 -5.9321156e-15 -9.4166677e-15\n",
      "  2.9718540e-16 -1.4957708e-15 -6.0970219e-15  3.0000000e+00], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 19 16 19 15 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0 11  0 13  6 13  8  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-3.8647284e-15  4.0000000e+00 -1.8271811e-15 -2.6271277e-16\n",
      " -1.6137992e-14  3.1182218e-16 -1.0679120e-15 -7.0504617e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 16 16 16  6 16 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 3 14  0  0  0  9 10  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-1.0700225e-14 -1.8932342e-14 -1.4285521e-14 -1.8271811e-15\n",
      " -1.3564553e-14  3.0000000e+00 -4.0829635e-15 -5.7360360e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 19 14 16 16  1 19 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[13  0 10 13 14  6 13  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-2.0607246e-14  6.1825308e-15  2.9155836e-14 -5.8861857e-15\n",
      " -4.1914333e-15 -2.0451803e-15  4.2475933e-15 -1.2082657e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 15 15 15 16 19 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[11 13  8  0  7  4  6 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-5.0893877e-15  3.0310860e-15  1.7468491e-15  2.0412463e-15\n",
      " -4.6185217e-15  7.0637090e-16 -2.0960012e-15 -9.7984475e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 14 16 16 16 19 15  6], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 6  0  8 11 14 18 14  3]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.00000000e+00  1.46500203e-15 -8.13948941e-15 -1.14636025e-14\n",
      "  2.00000000e+00  1.05830524e-14  6.67551377e-16  1.34545876e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([14 15  1 19 15 19 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 0  8  0 18  8 18 10 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 2.2469209e-14  3.2130255e-15 -8.6338898e-15  4.0000000e+00\n",
      "  2.5550330e-17  3.0000000e+00 -8.9601043e-15 -2.2668163e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([15 19 15 15  1  6 16 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 6 13  0 13  6  3  0  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 3.5549778e-15  1.5796735e-15  1.2126061e-15  1.0430701e-15\n",
      " -8.3576758e-15 -2.4262883e-16 -4.3234997e-15 -7.8904389e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([19  1 15 15  6 16 16 14], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[13  0  8  8  3 11  1  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 6.3583756e-15  4.0000000e+00  1.2138114e-16  9.0816099e-16\n",
      " -2.4262883e-16 -3.8823073e-15 -2.5977298e-15  1.4650020e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([19 16 16 19  6 14 14 15], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[ 4  0  0 13 10  0 11  6]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 7.0637090e-16 -4.1977852e-15 -7.3108794e-15  2.7375487e-15\n",
      " -1.7599178e-16  5.5822694e-15  8.5687623e-15  9.7079780e-16], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([16 15 19 15 16  1 16 12], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[14 11 13  8  8  0  0  0]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[-2.22703681e-14  1.15836298e-15  2.44724632e-15  1.60937287e-15\n",
      " -1.74283924e-14 -5.89505412e-15 -1.03776274e-14  3.19564609e-15], shape=(8,), dtype=float32)\n",
      "action\n",
      "tf.Tensor([19 15  1 15 16 16 15 16], shape=(8,), dtype=int32)\n",
      "optimal action\n",
      "[13  8  0  8  6  7  0 14]\n",
      "reward\n",
      "tf.Tensor(\n",
      "[ 1.0328985e-14  1.1403475e-15 -6.3513364e-15  2.5550330e-17\n",
      " -8.7178554e-15 -5.4043485e-15 -5.5878347e-17 -3.0423300e-15], shape=(8,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    time_step = env.reset()\n",
    "    action_step = a.policy.action(time_step)\n",
    "    time_step = env.step(action_step.action)\n",
    "    reward = time_step.reward\n",
    "    next_observation = time_step.observation\n",
    "    print(\"action\")\n",
    "    print(action_step.action)\n",
    "    print(\"optimal action\")\n",
    "    print(env.compute_optimal_action())\n",
    "    print(\"reward\")\n",
    "    print(reward)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reverb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11960/3687529474.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     replay_buffer_signature)\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m table = reverb.Table(\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[0mtable_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mmax_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreplay_buffer_max_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'reverb' is not defined"
     ]
    }
   ],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      a.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    a.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(19)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': TensorSpec(shape=(20,), dtype=tf.float64, name='observation'),\n",
      " 'policy_info': (),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "num_eval_episodes = 10\n",
    "\n",
    "# Reset the train step.\n",
    "a.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(env, a.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      a.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = a.train(experience).loss\n",
    "\n",
    "  step = a.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, a.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}