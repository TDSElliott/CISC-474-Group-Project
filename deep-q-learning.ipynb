{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (reverb.py, line 64)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001B[0;36m(most recent call last)\u001B[0m:\n",
      "  File \u001B[1;32m\"/Users/chrispop/Desktop/CISC-474-Group-Project/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001B[0m, line \u001B[1;32m3457\u001B[0m, in \u001B[1;35mrun_code\u001B[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/rc/v4xk3ffj5pd3_rqpm_z4m8vh0000gn/T/ipykernel_93135/413896743.py\"\u001B[0;36m, line \u001B[0;32m47\u001B[0;36m, in \u001B[0;35m<module>\u001B[0;36m\u001B[0m\n\u001B[0;31m    import reverb\u001B[0m\n",
      "\u001B[0;36m  File \u001B[0;32m\"/Users/chrispop/Desktop/CISC-474-Group-Project/venv/lib/python3.8/site-packages/reverb.py\"\u001B[0;36m, line \u001B[0;32m64\u001B[0m\n\u001B[0;31m    raise TypeError, 'Regexp cannot be negated'\u001B[0m\n\u001B[0m                   ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.saving.saved_model_experimental import sequential\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import constant as const\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Callable, Dict, List, Any, Optional, TypeVar\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.agents import TFAgent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import TFEnvironment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics.tf_metric import TFStepMetric\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "import tf_agents\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "T = TypeVar(\"T\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def define_rl_environment() -> TFEnvironment:\n",
    "    env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "    const.DATA_PATH, const.RANK_K, const.BATCH_SIZE, num_movies=const.NUM_ACTIONS, csv_delimiter=\"\\t\")\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "    return environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# env = define_rl_environment()\n",
    "# agent = define_rl_agent(env)\n",
    "# metric = define_rl_metric(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 23:19:25.387451: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "# agent = dqn_agent.DqnAgent(\n",
    "#     train_env.time_step_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     q_network=q_net,\n",
    "#     optimizer=optimizer,\n",
    "#     td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "#     train_step_counter=train_step_counter)\n",
    "#\n",
    "# agent.initialize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def define_rl_agent(environment: TFEnvironment) -> TFAgent:\n",
    "    # Define RL agent/algorithm.\n",
    "    agent = dqn_agent.DqnAgent(\n",
    "        time_step_spec=environment.time_step_spec(),\n",
    "        action_spec=environment.action_spec(),\n",
    "        #tikhonov_weight=const.TIKHONOV_WEIGHT,\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "        #alpha=const.AGENT_ALPHA,\n",
    "        #dtype=tf.float32,\n",
    "        #accepts_per_arm_features=const.PER_ARM)\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def define_rl_metric(environment: TFEnvironment) -> List[Any]:\n",
    "    optimal_reward_fn = functools.partial(\n",
    "    environment_utilities.compute_optimal_reward_with_movielens_environment,\n",
    "    environment=environment)\n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    metrics = [regret_metric]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    root_dir: str,\n",
    "    agent: TFAgent,\n",
    "    environment: TFEnvironment,\n",
    "    training_loops: int,\n",
    "    steps_per_loop: int,\n",
    "    additional_metrics: Optional[List[TFStepMetric]] = None,\n",
    "    training_data_spec_transformation_fn: Optional[Callable[[T], T]] = None,\n",
    ") -> Dict[str, List[float]]:\n",
    "  \"\"\"Performs `training_loops` iterations of training on the agent's policy.\n",
    "  Uses the `environment` as the problem formulation and source of immediate\n",
    "  feedback and the agent's algorithm, to perform `training-loops` iterations\n",
    "  of on-policy training on the policy.\n",
    "  If one or more baseline_reward_fns are provided, the regret is computed\n",
    "  against each one of them. Here is example baseline_reward_fn:\n",
    "  def baseline_reward_fn(observation, per_action_reward_fns):\n",
    "   rewards = ... # compute reward for each arm\n",
    "   optimal_action_reward = ... # take the maximum reward\n",
    "   return optimal_action_reward\n",
    "  Args:\n",
    "    root_dir: Path to the directory where training artifacts are written.\n",
    "    agent: An instance of `TFAgent`.\n",
    "    environment: An instance of `TFEnvironment`.\n",
    "    training_loops: An integer indicating how many training loops should be run.\n",
    "    steps_per_loop: An integer indicating how many driver steps should be\n",
    "      executed and presented to the trainer during each training loop.\n",
    "    additional_metrics: Optional; list of metric objects to log, in addition to\n",
    "      default metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n",
    "      `AverageEpisodeLengthMetric`.\n",
    "    training_data_spec_transformation_fn: Optional; function that transforms\n",
    "      the data items before they get to the replay buffer.\n",
    "  Returns:\n",
    "    A dict mapping metric names (eg. \"AverageReturnMetric\") to a list of\n",
    "    intermediate metric values over `training_loops` iterations of training.\n",
    "  \"\"\"\n",
    "  if training_data_spec_transformation_fn is None:\n",
    "    data_spec = agent.policy.trajectory_spec\n",
    "  else:\n",
    "    data_spec = training_data_spec_transformation_fn(\n",
    "        agent.policy.trajectory_spec)\n",
    "  replay_buffer = trainer.get_replay_buffer(data_spec, environment.batch_size,\n",
    "                                            steps_per_loop)\n",
    "\n",
    "  # `step_metric` records the number of individual rounds of bandit interaction;\n",
    "  # that is, (number of trajectories) * batch_size.\n",
    "  step_metric = tf_metrics.EnvironmentSteps()\n",
    "  metrics = [\n",
    "      tf_metrics.NumberOfEpisodes(),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n",
    "  ]\n",
    "  if additional_metrics:\n",
    "    metrics += additional_metrics\n",
    "\n",
    "  if isinstance(environment.reward_spec(), dict):\n",
    "    metrics += [tf_metrics.AverageReturnMultiMetric(\n",
    "        reward_spec=environment.reward_spec(),\n",
    "        batch_size=environment.batch_size)]\n",
    "  else:\n",
    "    metrics += [\n",
    "        tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)]\n",
    "\n",
    "  # Store intermediate metric results, indexed by metric names.\n",
    "  metric_results = defaultdict(list)\n",
    "\n",
    "  if training_data_spec_transformation_fn is not None:\n",
    "    add_batch_fn = lambda data: replay_buffer.add_batch(\n",
    "        training_data_spec_transformation_fn(data))\n",
    "  else:\n",
    "    add_batch_fn = replay_buffer.add_batch\n",
    "\n",
    "  observers = [add_batch_fn, step_metric] + metrics\n",
    "\n",
    "  driver = dynamic_step_driver.DynamicStepDriver(\n",
    "      env=environment,\n",
    "      policy=agent.collect_policy,\n",
    "      num_steps=steps_per_loop * environment.batch_size,\n",
    "      observers=observers)\n",
    "\n",
    "  training_loop = trainer.get_training_loop_fn(\n",
    "      driver, replay_buffer, agent, steps_per_loop)\n",
    "  saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "  for _ in range(training_loops):\n",
    "    training_loop()\n",
    "    metric_utils.log_metrics(metrics)\n",
    "    for metric in metrics:\n",
    "      metric.tf_summaries(train_step=step_metric.result())\n",
    "      metric_results[type(metric).__name__].append(metric.result().numpy())\n",
    "  saver.save(root_dir)\n",
    "  return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_observations_by_users(observation: List[List[float]]) -> List[int]:\n",
    "    # Trained policy is saved in the ROOT_DIR, given observation\n",
    "    # outputs action.\n",
    "    # https://www.tensorflow.org/agents/tutorials/\n",
    "    # 10_checkpointer_policysaver_tutorial#restore_checkpoint\n",
    "    trained_policy = tf.saved_model.load(const.ROOT_DIR)\n",
    "\n",
    "\n",
    "    # reference: https://github.com/yutsai84/vertex-ai-samples/\n",
    "    # blob/ee6dd357320a9fb875750331c2558b510c8b316f/community-content/\n",
    "    # tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/\n",
    "    # step_by_step_sdk_tf_agents_bandits_movie_recommendation/src/\n",
    "    # prediction/main.py#L60-L63\n",
    "\n",
    "    time_step = tf_agents.trajectories.restart(\n",
    "        observation=observation,\n",
    "        batch_size=tf.convert_to_tensor([const.BATCH_SIZE]),\n",
    "    )\n",
    "    action_step = trained_policy.action(time_step)\n",
    "    return action_step.action.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'create_variables'\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/rc/v4xk3ffj5pd3_rqpm_z4m8vh0000gn/T/ipykernel_93135/3773672319.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdefine_rl_environment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdefine_rl_agent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mmetric\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdefine_rl_metric\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/rc/v4xk3ffj5pd3_rqpm_z4m8vh0000gn/T/ipykernel_93135/1927840214.py\u001B[0m in \u001B[0;36mdefine_rl_agent\u001B[0;34m(environment)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mdefine_rl_agent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menvironment\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTFEnvironment\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTFAgent\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;31m# Define RL agent/algorithm.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     agent = dqn_agent.DqnAgent(\n\u001B[0m\u001B[1;32m      4\u001B[0m         \u001B[0mtime_step_spec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0menvironment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime_step_spec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0maction_spec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0menvironment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction_spec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/CISC-474-Group-Project/venv/lib/python3.8/site-packages/gin/config.py\u001B[0m in \u001B[0;36mgin_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1603\u001B[0m       \u001B[0mscope_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\" in scope '{}'\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscope_str\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mscope_str\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1604\u001B[0m       \u001B[0merr_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0merr_str\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn_or_cls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscope_info\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1605\u001B[0;31m       \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maugment_exception_message_and_reraise\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merr_str\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1606\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1607\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mgin_wrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/CISC-474-Group-Project/venv/lib/python3.8/site-packages/gin/utils.py\u001B[0m in \u001B[0;36maugment_exception_message_and_reraise\u001B[0;34m(exception, message)\u001B[0m\n\u001B[1;32m     39\u001B[0m   \u001B[0mproxy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mExceptionProxy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m   \u001B[0mExceptionProxy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__qualname__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__qualname__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m   \u001B[0;32mraise\u001B[0m \u001B[0mproxy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexception\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/CISC-474-Group-Project/venv/lib/python3.8/site-packages/gin/config.py\u001B[0m in \u001B[0;36mgin_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1580\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1581\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1582\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mnew_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mnew_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1583\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1584\u001B[0m       \u001B[0merr_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/CISC-474-Group-Project/venv/lib/python3.8/site-packages/tf_agents/agents/dqn/dqn_agent.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, name)\u001B[0m\n\u001B[1;32m    234\u001B[0m       net_observation_spec, _ = observation_and_action_constraint_splitter(\n\u001B[1;32m    235\u001B[0m           net_observation_spec)\n\u001B[0;32m--> 236\u001B[0;31m     \u001B[0mq_network\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet_observation_spec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    237\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mtarget_q_network\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    238\u001B[0m       \u001B[0mtarget_q_network\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet_observation_spec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Sequential' object has no attribute 'create_variables'\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)"
     ]
    }
   ],
   "source": [
    "env = define_rl_environment()\n",
    "agent = define_rl_agent(env)\n",
    "metric = define_rl_metric(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "TensorSpec(shape=(20,), dtype=tf.float64, name='observation')\n",
      "Reward spec\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "Action spec\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(19, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "\n",
    "print('Reward spec')\n",
    "print(env.reward_spec())\n",
    "\n",
    "print('Action spec')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/rc/v4xk3ffj5pd3_rqpm_z4m8vh0000gn/T/ipykernel_92327/1497556522.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconst\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mROOT_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconst\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTRAINING_LOOPS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconst\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSTEPS_PER_LOOP\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = train(const.ROOT_DIR, agent, env, const.TRAINING_LOOPS, const.STEPS_PER_LOOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actions = predict_observations_by_users(env.time_step_spec().observation)\n",
    "print(actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation:', env.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', env.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', env.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', env.time_step_spec().reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}